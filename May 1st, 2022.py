# -*- coding: utf-8 -*-
"""Welcome To Colaboratory

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/notebooks/intro.ipynb
"""

import pandas as pd
from sklearn.datasets import load_iris
from IPython import display
from sklearn.model_selection import train_test_split
from pandas.plotting import scatter_matrix
import mglearn
from sklearn.neighbors import KNeighborsClassifier
import numpy as np

df = load_iris()
df.keys()

print(df['data_module'])

X_train, X_test, y_train, y_test = train_test_split(
 df['data'], df['target'], random_state=0)

dft = pd.DataFrame(X_train, columns = df['feature_names'])
dft.head()

# c: color to differ; cmap: palette;
# s: size
# hist_kwds: hist keywords, where you can deliver the arguments of hist plot to change the hist in scatter_matrix
scatter_matrix(dft, c = y_train, figsize = (15,15), marker='o', hist_kwds = {"bins" : 20}, s = 60, alpha = .8, cmap=mglearn.cm3);

# From the plots, we can see that the three classes seem to be relatively well separated 
# using the sepal and petal measurements. This means that a machine learning model
# will likely be able to learn to separate them.
# 通过检查数据确定ML是否能用

knn = KNeighborsClassifier(n_neighbors=1)
knn.fit(X_train, y_train)
X_new = np.array([[5, 2.9, 1, 0.2]])
print("X_new.shape: {}".format(X_new.shape))

prediction = knn.predict(X_new)
print("Prediction: {}".format(prediction))

print("Predicted target name: {}".format(
 dft['target_names'][prediction]))

df['target_names']